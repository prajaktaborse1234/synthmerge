endpoints:

  - name: "Claude Sonnet 4.5"
    url: "https://api.anthropic.com/v1/messages"
    type: "anthropic"
    x_api_key_file: "~/.keys/anthropic.api-key"
    json:
      max_tokens: 20000
      model: "claude-sonnet-4-5"
      temperature: 0
    headers:
      anthropic-version: "2023-06-01"
    variants:
      - name: "default"
      - name: "no_diff"
        context:
          no_diff: true
      #- name: "userctx"
      #  context:
      #    layout:
      #      system_message:
      #        - prompt
      #      user_message:
      #        - training
      #        - diff

  - name: "Vertex Claude Sonnet 4.0"
    url: "https://host/path"
    type: "anthropic"
    api_key_file: "~/.keys/claude.api-key"
    json:
      anthropic_version: "something-YYYY-MM-DD"
      max_tokens: 20000
      temperature: 0
    variants:
      - name: "default"
      - name: "no_diff"
        context:
          no_diff: true
      #- name: "userctx"
      #  context:
      #    layout:
      #      system_message:
      #        - prompt
      #      user_message:
      #        - training
      #        - diff
    # Optional root certificate for HTTPS endpoints
    # root_certificate_pem: "~/.ssl/corp-ca.pem"

  - name: "Patchpal AI"
    type: "patchpal"
    url: "http://patchpal.usersys.redhat.com:9080/v1"

  - name: "Gemini 2.5 Flash"
    url: "https://generativelanguage.googleapis.com/v1beta/openai/chat/completions"
    type: "openai"
    api_key_file: "~/.keys/gemini.api-key"
    json:
      model: "gemini-2.5-flash"
      # "none" (only available with Flash) works better with default layout
      reasoning_effort: "none"
    variants:
      - name: "default"
      - name: "no_diff"
        context:
          no_diff: true

  - name: "Gemini 2.5 Pro"
    url: "https://generativelanguage.googleapis.com/v1beta/openai/chat/completions"
    type: "openai"
    api_key_file: "~/.keys/gemini.api-key"
    json:
      model: "gemini-2.5-pro"
      reasoning_effort: "low"
    context:
      # pro needs the prompt at the top of system_message
      layout:
        system_message:
          - prompt
        user_message:
          - training
          - diff
    variants:
      - name: "default"
      - name: "no_diff"
        context:
          no_diff: true

  - name: "Gemini 3 Pro preview"
    url: "https://generativelanguage.googleapis.com/v1beta/openai/chat/completions"
    type: "openai"
    api_key_file: "~/.keys/gemini.api-key"
    json:
      model: "gemini-3-pro-preview"
      reasoning_effort: "low"
    context:
      layout:
        system_message:
          - prompt
        user_message:
          - training
          - diff
    variants:
      - name: "default"
      - name: "no_diff"
        context:
          no_diff: true

  - name: "llama.cpp vulkan minimal" # requires --no-jinja
    url: "http://localhost:8811/v1/chat/completions"
    type: "openai"

  - name: "llama.cpp vulkan" # requires --no-jinja
    url: "http://localhost:8811/v1/chat/completions"
    #timeout: 600000
    #retries: 10
    #delay: 1000
    #max_delay: 600000
    #wait: 1000
    type: "openai"
    json:
      #temperature: 0.7
      #top_p: 0.8
      #top_k: 20
      #min_p: 0

      # n_probs: 1 provides the probability of the lowest probability
      # token in the resolved conflict
      n_probs: 1

      # n_probs: 2 same as n_probs: 1 but it also provides two more
      # beams with the perplexity search algorithm of synthmerge
      # applied to the logprobs, which is a client side only
      # approximated beam search
      #n_probs: 2
    variants:
      # one query for each entry in the variants list
      - name: "default"
      - name: "no_diff"
        context:
          no_diff: true
      #- name: "min_p"
      #  json:
      #    temperature: 0.3
      #    top_p: 1.0
      #    top_k: 0
      #    min_p: 0.9

  - name: "llama.cpp vulkan no_chat" # requires --no-jinja
    url: "http://localhost:8811/v1/completions"
    type: "openai"
    no_chat: true
    context:
      no_training: true
